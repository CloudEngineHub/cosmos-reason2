{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10157b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ec0ba",
   "metadata": {},
   "source": [
    "# GRPO Cosmos-Reason2 with QLoRA using TRL\n",
    "\n",
    "**WORK IN PROGRESS!**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nvidia-cosmos/cosmos-reason2/blob/main/examples/notebooks/trl_grpo.ipynb)\n",
    "\n",
    "- [TRL GitHub Repository](https://github.com/huggingface/trl)\n",
    "- [Official TRL Examples](https://huggingface.co/docs/trl/example_overview)  \n",
    "- [Community Tutorials](https://huggingface.co/docs/trl/community_tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f72b8",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "We'll install **TRL** with the **PEFT** extra, which ensures all main dependencies such as **Transformers** and **PEFT** (a package for parameter-efficient fine-tuning, e.g., LoRA/QLoRA) are included. Additionally, we'll install **bitsandbytes** to enable quantization of LLMs, reducing memory consumption for both inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949061be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq \"trl[peft]==0.26.1\" \"bitsandbytes==0.49.0\" \"tensorboard==2.20.0\" \"math_verify==0.8.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1384e8",
   "metadata": {},
   "source": [
    "### Log in to Hugging Face\n",
    "\n",
    "Log in to your **Hugging Face** account to save your fine-tuned model, track your experiment results directly on the Hub or access gated models. You can find your **access token** on your [account settings page](https://huggingface.co/settings/tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb085bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb6868",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "\n",
    "We'll load the [**lmms-lab/multimodal-open-r1-8k-verified**](https://huggingface.co/datasets/lmms-lab/multimodal-open-r1-8k-verified) dataset from the Hugging Face Hub using the `datasets` library.\n",
    "\n",
    "This dataset contains maths problems with the image representing the problem,  along with the solution in thinking format specially tailored for VLMs. By training our model with this dataset, it'll improve its maths and thinking reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = 'lmms-lab/multimodal-open-r1-8k-verified'\n",
    "train_dataset = load_dataset(dataset_id, split='train[:5%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76fc29",
   "metadata": {},
   "source": [
    "In addition to the `problem` and `image` columns, we also include a custom system prompt to tell the model how we'd like the generation.\n",
    "\n",
    "The system prompt is extracted from DeepSeek R1. Refer to [this previous recipe](https://huggingface.co/learn/cookbook/fine_tuning_llm_grpo_trl) for more details.\n",
    "\n",
    "We convert the dataset samples into conversation samples, including the system prompt and one image and problem description per sample, since this is how the GRPO trainer expects them.\n",
    "\n",
    "We also set `padding_side=\"left\"` to ensure that generated completions during training are concatenated directly after the prompt, which is essential for GRPO to correctly compare token-level probabilities between preferred and rejected responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "model_name = \"nvidia/Cosmos-Reason2-2B\"\n",
    "processor = AutoProcessor.from_pretrained(model_name, padding_side=\"left\")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant.\n",
    "\n",
    "Answer the question using the following format:\n",
    "\n",
    "<think>\n",
    "Your reasoning.\n",
    "</think>\n",
    "\n",
    "Write your final answer immediately after the </think> tag.\"\"\"\n",
    "\n",
    "\n",
    "def make_conversation(example):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": example[\"image\"]},\n",
    "                {\"type\": \"text\", \"text\": example[\"problem\"]},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"image\": example[\"image\"],\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(make_conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b2a6c",
   "metadata": {},
   "source": [
    "Let's review one example to understand the internal structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(['problem', 'original_question', 'original_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79414a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81823c",
   "metadata": {},
   "source": [
    "## Load model and configure LoRA/QLoRA\n",
    "\n",
    "This notebook can be used with two fine-tuning methods. By default, it is set up for **QLoRA**, which includes quantization using `BitsAndBytesConfig`. If you prefer to use standard **LoRA** without quantization, simply comment out the `BitsAndBytesConfig` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    model_name, dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad271a",
   "metadata": {},
   "source": [
    "The following cell defines LoRA (or QLoRA if needed). When training with LoRA/QLoRA, we use a **base model** (the one selected above) and, instead of modifying its original weights, we fine-tune a **LoRA adapter** â€” a lightweight layer that enables efficient and memory-friendly training. The **`target_modules`** specify which parts of the model (e.g., attention or projection layers) will be adapted by LoRA during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b833b8",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "We'll configure **GRPO** using `GRPOConfig`, keeping the parameters minimal. You can adjust these settings if more resources are available. For full details on all available parameters, check the [TRL GRPOConfig documentation](https://huggingface.co/docs/trl/sft_trainer#trl.GRPOConfig).\n",
    "\n",
    "First, we need to define the rewards functions that the training algorithm will use to improve the model. In this case, we'll include two reward functions.\n",
    "We'll use a format reward that will reward the model when the output includes `<think>` and `<answer>` tags and additionally a length-based reward to discourage overthinking. Both functions have been extracted from [here](https://github.com/huggingface/open-r1/blob/main/src/open_r1/rewards.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899b16c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the reasoning process is enclosed within <think> and </think> tags, while the final answer is enclosed within <answer> and </answer> tags.\"\"\"\n",
    "    pattern = r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>$\"\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE) for content in completions]\n",
    "    return [1.0 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "\n",
    "\n",
    "def len_reward(completions, solution, **kwargs) -> float:\n",
    "    \"\"\"Compute length-based rewards to discourage overthinking and promote token efficiency.\n",
    "\n",
    "    Taken from the Kimi 1.5 tech report: https://huggingface.co/papers/2501.12599\n",
    "\n",
    "    Args:\n",
    "        completions: List of model completions\n",
    "        solution: List of ground truth solutions\n",
    "\n",
    "    Returns:\n",
    "        List of rewards where:\n",
    "        - For correct answers: reward = 0.5 - (len - min_len)/(max_len - min_len)\n",
    "        - For incorrect answers: reward = min(0, 0.5 - (len - min_len)/(max_len - min_len))\n",
    "    \"\"\"\n",
    "    contents = completions\n",
    "\n",
    "    # First check correctness of answers\n",
    "    correctness = []\n",
    "    for content, sol in zip(contents, solution):\n",
    "        gold_parsed = parse(\n",
    "            sol,\n",
    "            extraction_mode=\"first_match\",\n",
    "            extraction_config=[LatexExtractionConfig()],\n",
    "        )\n",
    "        if len(gold_parsed) == 0:\n",
    "            # Skip unparsable examples\n",
    "            correctness.append(True)  # Treat as correct to avoid penalizing\n",
    "            print(\"Failed to parse gold solution: \", sol)\n",
    "            continue\n",
    "\n",
    "        answer_parsed = parse(\n",
    "            content,\n",
    "            extraction_config=[\n",
    "                LatexExtractionConfig(\n",
    "                    normalization_config=NormalizationConfig(\n",
    "                        nits=False,\n",
    "                        malformed_operators=False,\n",
    "                        basic_latex=True,\n",
    "                        equations=True,\n",
    "                        boxed=True,\n",
    "                        units=True,\n",
    "                    ),\n",
    "                    boxed_match_priority=0,\n",
    "                    try_extract_without_anchor=False,\n",
    "                )\n",
    "            ],\n",
    "            extraction_mode=\"first_match\",\n",
    "        )\n",
    "        correctness.append(verify(answer_parsed, gold_parsed))\n",
    "\n",
    "    # Calculate lengths\n",
    "    lengths = [len(content) for content in contents]\n",
    "    min_len = min(lengths)\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    # If all responses have the same length, return zero rewards\n",
    "    if max_len == min_len:\n",
    "        return [0.0] * len(completions)\n",
    "\n",
    "    rewards = []\n",
    "    for length, is_correct in zip(lengths, correctness):\n",
    "        lambda_val = 0.5 - (length - min_len) / (max_len - min_len)\n",
    "\n",
    "        if is_correct:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = min(0, lambda_val)\n",
    "\n",
    "        rewards.append(float(reward))\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa977a4",
   "metadata": {},
   "source": [
    "After defining the reward function(s), we can define the `GRPOConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd195ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "\n",
    "output_dir = \"outputs/Cosmos-Reason2-2B-trl-grpo\"\n",
    "\n",
    "# Configure training arguments using GRPOConfig\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate=2e-5,\n",
    "    #num_train_epochs=1,\n",
    "    max_steps=100,                                        # Number of dataset passes. For full trainings, use `num_train_epochs` instead\n",
    "\n",
    "    # Parameters that control the data preprocessing\n",
    "    per_device_train_batch_size=2,\n",
    "    max_completion_length=1024, # default: 256            # Max completion length produced during training\n",
    "    num_generations=2, # 2, # default: 8                  # Number of generations produced during training for comparison\n",
    "\n",
    "    fp16=True,\n",
    "\n",
    "    # Parameters related to reporting and saving\n",
    "    output_dir=output_dir,                                # Where to save model checkpoints and logs\n",
    "    logging_steps=1,                                      # Log training metrics every N steps\n",
    "    report_to=\"tensorboard\",                                  # Experiment tracking tool\n",
    "\n",
    "    # Hub integration\n",
    "    push_to_hub=True,\n",
    "    log_completions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef287a",
   "metadata": {},
   "source": [
    "Configure the GRPO Trainer. We pass the previously configured `training_args`. We don't use eval dataset to maintain memory usage low but you can configure it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[format_reward, len_reward],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78f04a",
   "metadata": {},
   "source": [
    "Show memory stats before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1e2ed",
   "metadata": {},
   "source": [
    "And train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe5f42",
   "metadata": {},
   "source": [
    "Show memory stats after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe69fe7",
   "metadata": {},
   "source": [
    "## Saving fine tuned model\n",
    "\n",
    "In this step, we save the fine-tuned model both **locally** and to the **Hugging Face Hub** using the credentials from your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "trainer.push_to_hub(dataset_name=dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c9312",
   "metadata": {},
   "source": [
    "## Load the fine-tuned model and run inference\n",
    "\n",
    "Now, let's test our fine-tuned model by loading the **LoRA/QLoRA adapter** and performing **inference**. We'll start by loading the **base model**, then attach the adapter to it, creating the final fine-tuned model ready for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = model_name\n",
    "adapter_model = f\"{output_dir}\" # Replace with your HF username or organization\n",
    "\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(base_model, dtype=\"auto\", device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, adapter_model)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7528c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = 'lmms-lab/multimodal-open-r1-8k-verified'\n",
    "train_dataset = load_dataset(dataset_id, split='train[:5%]')\n",
    "\n",
    "problem = train_dataset[0]['problem']\n",
    "image = train_dataset[0]['image']\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": SYSTEM_PROMPT}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": problem},\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
